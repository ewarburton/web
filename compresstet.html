<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EDO Audio Converter (Alpha 1.0)</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #1a1a1a;
            color: #e0e0e0;
        }
        h1 {
            text-align: center;
            color: #4a9eff;
        }
        .container {
            background: #2a2a2a;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        .input-group {
            margin: 20px 0;
        }
        label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: #b0b0b0;
        }
        input[type="file"],
        input[type="number"] {
            width: 100%;
            padding: 10px;
            border: 2px solid #444;
            border-radius: 5px;
            font-size: 16px;
            background: #1a1a1a;
            color: #e0e0e0;
            box-sizing: border-box;
        }
        input[type="checkbox"] {
            width: 20px;
            height: 20px;
            cursor: pointer;
            accent-color: #4a9eff;
        }
        input[type="number"]:focus,
        input[type="file"]:focus {
            outline: none;
            border-color: #4a9eff;
        }
        .edo-inputs {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
        }
        button {
            width: 100%;
            padding: 15px;
            background: #4a9eff;
            color: white;
            border: none;
            border-radius: 5px;
            font-size: 18px;
            font-weight: bold;
            cursor: pointer;
            margin-top: 20px;
            transition: background 0.3s;
        }
        button:hover {
            background: #3a7ecf;
        }
        button:disabled {
            background: #555;
            cursor: not-allowed;
        }
        video {
            width: 100%;
            margin: 20px 0;
            border-radius: 5px;
            display: none;
        }
        video.active {
            display: block;
        }
        .video-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 20px 0;
        }
        .video-wrapper {
            display: none;
        }
        .video-wrapper.active {
            display: block;
        }
        .video-wrapper label {
            font-size: 14px;
            color: #4a9eff;
            margin-bottom: 5px;
        }
        #colorVideoCanvas {
            display: none;
        }
        .spectrogram-container {
            margin: 20px 0;
            display: none;
        }
        .spectrogram-container.active {
            display: block;
        }
        .spectrogram-label {
            font-weight: 600;
            color: #4a9eff;
            margin-bottom: 5px;
        }
        canvas {
            width: 100%;
            height: 200px;
            background: #000;
            border-radius: 5px;
            margin-bottom: 15px;
            image-rendering: pixelated;
        }
        .status {
            margin-top: 20px;
            padding: 15px;
            background: #333;
            border-radius: 5px;
            display: none;
        }
        .status.active {
            display: block;
        }
        .status.processing {
            border-left: 4px solid #ff9800;
        }
        .status.success {
            border-left: 4px solid #4caf50;
        }
        .status.error {
            border-left: 4px solid #f44336;
        }
        .info {
            margin-top: 30px;
            padding: 15px;
            background: #333;
            border-radius: 5px;
            border-left: 4px solid #4a9eff;
        }
        .info h3 {
            margin-top: 0;
            color: #4a9eff;
        }
        .info p {
            margin: 8px 0;
            line-height: 1.6;
        }
    </style>
</head>
<body>
    <h1>EDO Audio Converter (Alpha 1.0)</h1>
    
    <div class="container">
        <div class="input-group">
            <label for="videoInput">Select Video File:</label>
            <input type="file" id="videoInput" accept="video/*">
        </div>

        <div class="video-container">
            <div class="video-wrapper" id="originalVideoWrapper">
                <label>Original Video:</label>
                <video id="videoPlayer" controls></video>
            </div>
            <div class="video-wrapper" id="colorVideoWrapper">
                <label>Color-Transformed Video (Hue = Pitch):</label>
                <video id="colorVideo" controls></video>
            </div>
        </div>
        <canvas id="colorVideoCanvas"></canvas>

        <div class="spectrogram-container" id="spectrogramContainer">
            <div class="spectrogram-label">Original Spectrogram:</div>
            <canvas id="originalSpectrogram"></canvas>
            <div class="spectrogram-label">Converted Spectrogram:</div>
            <canvas id="convertedSpectrogram"></canvas>
        </div>

        <div class="input-group">
            <label for="initialHz">Initial Hz (Reference Frequency):</label>
            <input type="number" id="initialHz" value="440" step="0.1" min="1">
        </div>

        <div class="input-group">
            <label for="scaleMaker">Scale Maker (comma-separated cents, optional):</label>
            <input type="text" id="scaleMaker" placeholder="e.g., 0,100,200,300,400,500,600,700,800,900,1000,1100,1200" style="width: 100%; padding: 10px; border: 2px solid #444; border-radius: 5px; font-size: 16px; background: #1a1a1a; color: #e0e0e0; box-sizing: border-box;">
            <small style="color: #888; margin-top: 5px; display: block;">Leave empty to use EDO. Enter scale steps in cents (e.g., "0,204,408,612,816,1020,1224" for Pythagorean JI whole tones)</small>
        </div>

        <div class="input-group">
            <label>Interval Mapping (optional):</label>
            <div id="intervalMapContainer">
                <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin-bottom: 10px;">
                    <small style="color: #888;">If cents &gt; than</small>
                    <small style="color: #888;">and cents &lt; than</small>
                    <small style="color: #888;">turn to (cents)</small>
                </div>
                <div id="intervalMapRows"></div>
                <button type="button" id="addIntervalRow" style="width: auto; padding: 8px 15px; margin-top: 10px; background: #555; font-size: 14px;">+ Add Interval</button>
            </div>
            <small style="color: #888; margin-top: 5px; display: block;">Define interval ranges to map to specific cents values. Ranges are checked before scale/EDO mapping.</small>
        </div>

        <div class="edo-inputs">
            <div class="input-group">
                <label for="fromTet">Change from (EDO):</label>
                <input type="number" id="fromTet" value="12" min="1" step="1">
            </div>
            <div class="input-group">
                <label for="toTet">To (EDO):</label>
                <input type="number" id="toTet" value="19" min="1" step="1">
            </div>
        </div>

        <div class="input-group">
            <label for="mode">Mode:</label>
            <select id="mode" style="width: 100%; padding: 10px; border: 2px solid #444; border-radius: 5px; font-size: 16px; background: #1a1a1a; color: #e0e0e0;">
                <option value="approximate">Approximate - Round to nearest pitch</option>
                <option value="scale">Scale - Transform by ratio (10-14 EDO only)</option>
            </select>
        </div>

        <div class="input-group" style="display: flex; align-items: center; gap: 10px;">
            <input type="checkbox" id="audioOnly" style="width: auto; margin: 0;">
            <label for="audioOnly" style="margin: 0; cursor: pointer;">Audio only (skip color video processing)</label>
        </div>

        <div class="input-group">
            <label for="frameSkip">Video Frame Processing (higher = faster):</label>
            <select id="frameSkip" style="width: 100%; padding: 10px; border: 2px solid #444; border-radius: 5px; font-size: 16px; background: #1a1a1a; color: #e0e0e0;">
                <option value="1">Process every frame (slowest, best quality)</option>
                <option value="2" selected>Process every 2nd frame (2x faster)</option>
                <option value="3">Process every 3rd frame (3x faster)</option>
                <option value="5">Process every 5th frame (5x faster)</option>
                <option value="10">Process every 10th frame (10x faster)</option>
            </select>
        </div>

        <button id="processBtn" disabled>Process Audio</button>

        <div id="status" class="status"></div>

        <div class="info">
            <h3>About This Tool</h3>
            <p><strong>How it works:</strong> This tool uses FFT with logarithmic frequency scaling to better match musical perception. It detects clusters of nearby frequencies (within 20 cents) using exponential weighting, then applies the selected mode.</p>
            <p><strong>Initial Hz:</strong> The reference frequency (typically A4 = 440 Hz).</p>
            <p><strong>Scale Maker:</strong> Optional custom scale in cents. Enter comma-separated values (e.g., "0,100,200,300,400,500,600,700,800,900,1000,1100,1200" for 12-EDO). Leave empty to use standard EDO. The scale automatically repeats each octave.</p>
            <p><strong>Interval Mapping:</strong> Optional conditional mapping. For each row, if a note falls within the range (greater than X cents AND less than Y cents), it will be transformed to Z cents. Mappings are applied before EDO/scale conversion. Works within each octave (0-1200 cents). Useful for custom pitch corrections or microtuning adjustments.</p>
            <p><strong>From EDO:</strong> Source equal temperament (default 12-EDO).</p>
            <p><strong>To EDO:</strong> Target equal temperament (e.g., 19-EDO, 24-EDO, 31-EDO, etc.). Ignored if using custom scale.</p>
            <p><strong>Modes:</strong></p>
            <p style="margin-left: 20px;"><strong>Approximate:</strong> Rounds each cluster to the nearest pitch in the target EDO or custom scale. Works for all conversions.</p>
            <p style="margin-left: 20px;"><strong>Scale:</strong> Transforms frequencies by the ratio fromEDO/toEDO. Converts 12x semitones to (fromEDO/toEDO) × 12x semitones. Only works when both source and target are 10-14 EDO. Ignores custom scale.</p>
            <p><strong>Process:</strong> The spectrum is converted to logarithmic scale. Clusters within 20 cents are detected with exponential magnitude weighting. Each cluster is amplified logarithmically (+10dB for 10 frequencies, +20dB for 100) and processed according to the selected mode.</p>
        </div>
    </div>

    <script>
        const videoInput = document.getElementById('videoInput');
        const videoPlayer = document.getElementById('videoPlayer');
        const initialHzInput = document.getElementById('initialHz');
        const scaleMakerInput = document.getElementById('scaleMaker');
        const fromTetInput = document.getElementById('fromTet');
        const toTetInput = document.getElementById('toTet');
        const modeSelect = document.getElementById('mode');
        const processBtn = document.getElementById('processBtn');
        const statusDiv = document.getElementById('status');
        const spectrogramContainer = document.getElementById('spectrogramContainer');
        const originalCanvas = document.getElementById('originalSpectrogram');
        const convertedCanvas = document.getElementById('convertedSpectrogram');

        let audioContext;
        let videoFile;
        let intervalMapRows = [];

        // Interval mapping row management
        const intervalMapRowsContainer = document.getElementById('intervalMapRows');
        const addIntervalRowBtn = document.getElementById('addIntervalRow');

        function createIntervalRow() {
            const rowId = Date.now();
            const rowDiv = document.createElement('div');
            rowDiv.id = `interval-row-${rowId}`;
            rowDiv.style.cssText = 'display: grid; grid-template-columns: 1fr 1fr 1fr auto; gap: 10px; margin-bottom: 8px; align-items: center;';
            
            const greaterInput = document.createElement('input');
            greaterInput.type = 'number';
            greaterInput.step = '0.1';
            greaterInput.placeholder = 'e.g., 0';
            greaterInput.style.cssText = 'padding: 8px; border: 2px solid #444; border-radius: 5px; background: #1a1a1a; color: #e0e0e0;';
            greaterInput.className = 'interval-greater';
            
            const lessInput = document.createElement('input');
            lessInput.type = 'number';
            lessInput.step = '0.1';
            lessInput.placeholder = 'e.g., 100';
            lessInput.style.cssText = 'padding: 8px; border: 2px solid #444; border-radius: 5px; background: #1a1a1a; color: #e0e0e0;';
            lessInput.className = 'interval-less';
            
            const turnToInput = document.createElement('input');
            turnToInput.type = 'number';
            turnToInput.step = '0.1';
            turnToInput.placeholder = 'e.g., 50';
            turnToInput.style.cssText = 'padding: 8px; border: 2px solid #444; border-radius: 5px; background: #1a1a1a; color: #e0e0e0;';
            turnToInput.className = 'interval-turnto';
            
            const removeBtn = document.createElement('button');
            removeBtn.textContent = '×';
            removeBtn.type = 'button';
            removeBtn.style.cssText = 'width: 40px; padding: 8px; background: #d32f2f; color: white; font-size: 18px; font-weight: bold;';
            removeBtn.onclick = () => {
                rowDiv.remove();
                intervalMapRows = intervalMapRows.filter(r => r.id !== rowId);
            };
            
            rowDiv.appendChild(greaterInput);
            rowDiv.appendChild(lessInput);
            rowDiv.appendChild(turnToInput);
            rowDiv.appendChild(removeBtn);
            
            intervalMapRowsContainer.appendChild(rowDiv);
            intervalMapRows.push({ id: rowId, element: rowDiv });
        }

        addIntervalRowBtn.addEventListener('click', createIntervalRow);

        // Initialize with one row
        createIntervalRow();

        // Parse interval mappings from UI
        function getIntervalMappings() {
            const mappings = [];
            const rows = intervalMapRowsContainer.querySelectorAll('[id^="interval-row-"]');
            
            rows.forEach(row => {
                const greaterInput = row.querySelector('.interval-greater');
                const lessInput = row.querySelector('.interval-less');
                const turnToInput = row.querySelector('.interval-turnto');
                
                const greaterThan = parseFloat(greaterInput.value);
                const lessThan = parseFloat(lessInput.value);
                const turnTo = parseFloat(turnToInput.value);
                
                if (!isNaN(greaterThan) && !isNaN(lessThan) && !isNaN(turnTo)) {
                    mappings.push({
                        greaterThan: greaterThan,
                        lessThan: lessThan,
                        turnTo: turnTo
                    });
                }
            });
            
            // Sort by greaterThan value for proper range checking
            mappings.sort((a, b) => a.greaterThan - b.greaterThan);
            
            return mappings;
        }

        // Apply interval mapping to cents value
        function applyIntervalMapping(centsFromRef, intervalMappings) {
            // Normalize cents to be within an octave (0-1200)
            const octave = Math.floor(centsFromRef / 1200);
            const centsInOctave = centsFromRef - (octave * 1200);
            
            // Check each interval mapping
            for (const mapping of intervalMappings) {
                if (centsInOctave > mapping.greaterThan && centsInOctave < mapping.lessThan) {
                    // Found a matching range, return the target value (preserving octave)
                    return octave * 1200 + mapping.turnTo;
                }
            }
            
            // No mapping found, return original value
            return centsFromRef;
        }

        // RGB to HSV conversion
        function rgbToHsv(r, g, b) {
            r /= 255;
            g /= 255;
            b /= 255;
            
            const max = Math.max(r, g, b);
            const min = Math.min(r, g, b);
            const diff = max - min;
            
            let h = 0;
            const s = max === 0 ? 0 : diff / max;
            const v = max;
            
            if (diff !== 0) {
                if (max === r) {
                    h = ((g - b) / diff + (g < b ? 6 : 0)) / 6;
                } else if (max === g) {
                    h = ((b - r) / diff + 2) / 6;
                } else {
                    h = ((r - g) / diff + 4) / 6;
                }
            }
            
            return { h: h * 360, s: s, v: v };
        }

        // HSV to RGB conversion
        function hsvToRgb(h, s, v) {
            h = h / 360;
            
            const i = Math.floor(h * 6);
            const f = h * 6 - i;
            const p = v * (1 - s);
            const q = v * (1 - f * s);
            const t = v * (1 - (1 - f) * s);
            
            let r, g, b;
            switch (i % 6) {
                case 0: r = v; g = t; b = p; break;
                case 1: r = q; g = v; b = p; break;
                case 2: r = p; g = v; b = t; break;
                case 3: r = p; g = q; b = v; break;
                case 4: r = t; g = p; b = v; break;
                case 5: r = v; g = p; b = q; break;
            }
            
            return {
                r: Math.round(r * 255),
                g: Math.round(g * 255),
                b: Math.round(b * 255)
            };
        }

        // Transform cents value using the same logic as audio processing
        function transformCents(cents, intervalMappings, customScale, fromEdo, toEdo, initialHz, mode) {
            const inValidRange = (fromEdo >= 10 && fromEdo <= 14 && toEdo >= 10 && toEdo <= 14);
            
            // Convert cents to frequency
            let freq = initialHz * Math.pow(2, cents / 1200);
            
            // Apply interval mapping if any
            let processedFreq = freq;
            if (intervalMappings.length > 0) {
                const centsFromRef = 1200 * Math.log2(freq / initialHz);
                const mappedCents = applyIntervalMapping(centsFromRef, intervalMappings);
                processedFreq = initialHz * Math.pow(2, mappedCents / 1200);
            }
            
            let targetFreq;
            if (mode === 'approximate' || !inValidRange) {
                if (customScale) {
                    targetFreq = findNearestScalePitch(processedFreq, initialHz, customScale);
                } else {
                    const targetStep = freqToEdoStep(processedFreq, initialHz, toEdo);
                    targetFreq = edoStepToFreq(targetStep, initialHz, toEdo);
                }
            } else if (mode === 'scale') {
                const semitones = 12 * Math.log2(processedFreq / initialHz);
                const scaledSemitones = semitones * (fromEdo / toEdo);
                targetFreq = initialHz * Math.pow(2, scaledSemitones / 12);
            }
            
            // Convert back to cents
            return 1200 * Math.log2(targetFreq / initialHz);
        }

        // Initialize Audio Context
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
        }

        // Handle video file selection
        videoInput.addEventListener('change', (e) => {
            const file = e.target.files[0];
            if (file) {
                videoFile = file;
                const url = URL.createObjectURL(file);
                videoPlayer.src = url;
                document.getElementById('originalVideoWrapper').classList.add('active');
                processBtn.disabled = false;
                showStatus('Video loaded. Ready to process.', 'success');
            }
        });

        // Show status message
        function showStatus(message, type = 'processing') {
            statusDiv.textContent = message;
            statusDiv.className = `status active ${type}`;
        }

        // Convert frequency to nearest pitch in an EDO system
        function freqToEdoStep(freq, referenceHz, edo) {
            const ratio = freq / referenceHz;
            const steps = Math.log2(ratio) * edo;
            return Math.round(steps);
        }

        // Convert EDO step back to frequency
        function edoStepToFreq(step, referenceHz, edo) {
            return referenceHz * Math.pow(2, step / edo);
        }

        // Find nearest pitch in custom scale (in cents)
        function findNearestScalePitch(freq, referenceHz, scaleCents) {
            // Convert frequency to cents from reference
            const cents = 1200 * Math.log2(freq / referenceHz);
            
            // Find octave and position within octave
            const octave = Math.floor(cents / 1200);
            const centsInOctave = cents - (octave * 1200);
            
            // Find nearest scale degree
            let minDiff = Infinity;
            let nearestCents = 0;
            
            for (const scaleCent of scaleCents) {
                const diff = Math.abs(centsInOctave - scaleCent);
                if (diff < minDiff) {
                    minDiff = diff;
                    nearestCents = scaleCent;
                }
            }
            
            // Convert back to frequency
            const totalCents = octave * 1200 + nearestCents;
            return referenceHz * Math.pow(2, totalCents / 1200);
        }

        // Draw spectrogram
        function drawSpectrogram(ctx, specData, width, height) {
            if (specData.length === 0) return;
            
            const timeSteps = specData.length;
            const freqBins = specData[0].length;
            
            // Find max magnitude for normalization
            let maxMag = 0;
            for (let t = 0; t < timeSteps; t++) {
                for (let f = 0; f < freqBins; f++) {
                    maxMag = Math.max(maxMag, specData[t][f]);
                }
            }
            
            // Draw spectrogram
            const pixelWidth = Math.max(1, width / timeSteps);
            const pixelHeight = Math.max(1, height / freqBins);
            
            for (let t = 0; t < timeSteps; t++) {
                for (let f = 0; f < freqBins; f++) {
                    const magnitude = specData[t][f];
                    const normalized = magnitude / (maxMag || 1);
                    
                    // Use log scale for better visibility
                    const intensity = Math.pow(normalized, 0.3) * 255;
                    
                    // Color mapping: blue (low) -> cyan -> yellow -> red (high)
                    let r, g, b;
                    if (intensity < 64) {
                        r = 0;
                        g = 0;
                        b = intensity * 4;
                    } else if (intensity < 128) {
                        r = 0;
                        g = (intensity - 64) * 4;
                        b = 255;
                    } else if (intensity < 192) {
                        r = (intensity - 128) * 4;
                        g = 255;
                        b = 255 - (intensity - 128) * 4;
                    } else {
                        r = 255;
                        g = 255 - (intensity - 192) * 4;
                        b = 0;
                    }
                    
                    ctx.fillStyle = `rgb(${r},${g},${b})`;
                    const x = (t / timeSteps) * width;
                    const y = height - ((f / freqBins) * height); // Flip Y axis
                    ctx.fillRect(x, y, Math.ceil(pixelWidth) + 1, Math.ceil(pixelHeight) + 1);
                }
            }
        }

        // Process audio with frequency separation and approximation
        async function processAudio() {
            try {
                initAudioContext();
                
                const fromEdo = parseInt(fromTetInput.value);
                const toEdo = parseInt(toTetInput.value);
                const initialHz = parseFloat(initialHzInput.value);
                
                // Get interval mappings
                const intervalMappings = getIntervalMappings();
                
                // Parse custom scale if provided
                let customScale = null;
                const scaleInput = scaleMakerInput.value.trim();
                if (scaleInput) {
                    try {
                        customScale = scaleInput.split(',').map(s => parseFloat(s.trim())).filter(n => !isNaN(n));
                        if (customScale.length === 0) {
                            customScale = null;
                        } else {
                            // Ensure scale starts at 0 and includes octave (1200)
                            if (!customScale.includes(0)) customScale.unshift(0);
                            if (!customScale.includes(1200)) customScale.push(1200);
                            customScale.sort((a, b) => a - b);
                            showStatus(`Using custom scale with ${customScale.length} steps`, 'processing');
                        }
                    } catch (e) {
                        showStatus('Invalid scale format, using EDO instead', 'processing');
                        customScale = null;
                    }
                }

                showStatus('Extracting audio from video...', 'processing');
                processBtn.disabled = true;

                // Decode video audio
                const arrayBuffer = await videoFile.arrayBuffer();
                const audioBuffer = await decodeAudioFromVideo(arrayBuffer);

                if (!audioBuffer) {
                    throw new Error('Could not extract audio from video');
                }

                showStatus('Analyzing frequencies...', 'processing');

                // Setup spectrograms
                spectrogramContainer.classList.add('active');
                const spectrogramWidth = 800;
                const spectrogramHeight = 200;
                originalCanvas.width = spectrogramWidth;
                originalCanvas.height = spectrogramHeight;
                convertedCanvas.width = spectrogramWidth;
                convertedCanvas.height = spectrogramHeight;
                
                const origCtx = originalCanvas.getContext('2d');
                const convCtx = convertedCanvas.getContext('2d');
                
                // Clear canvases
                origCtx.fillStyle = '#000';
                origCtx.fillRect(0, 0, spectrogramWidth, spectrogramHeight);
                convCtx.fillStyle = '#000';
                convCtx.fillRect(0, 0, spectrogramWidth, spectrogramHeight);

                // Process each channel
                const numChannels = audioBuffer.numberOfChannels;
                const sampleRate = audioBuffer.sampleRate;
                const length = audioBuffer.length;
                const outputChannels = [];
                
                // Store spectrogram data
                const originalSpecData = [];
                const convertedSpecData = [];

                for (let channel = 0; channel < numChannels; channel++) {
                    const channelData = audioBuffer.getChannelData(channel);
                    const outputData = new Float32Array(length);
                    
                    // Process in chunks using FFT
                    const fftSize = 4096;
                    const hopSize = fftSize / 4; // 75% overlap for better time resolution
                    const maxFreqDisplay = 8000; // Display up to 8kHz in spectrogram
                    
                    showStatus(`Processing channel ${channel + 1}/${numChannels}...`, 'processing');
                    
                    let frameIndex = 0;
                    const totalFrames = Math.floor(length / hopSize);
                    const framesPerPixel = Math.max(1, Math.floor(totalFrames / spectrogramWidth));
                    
                    for (let i = 0; i < length; i += hopSize) {
                        const chunk = new Float32Array(fftSize);
                        
                        // Extract chunk with Hann window
                        for (let j = 0; j < fftSize; j++) {
                            const idx = i + j;
                            if (idx < length) {
                                // Apply Hann window
                                const window = 0.5 * (1 - Math.cos(2 * Math.PI * j / fftSize));
                                chunk[j] = channelData[idx] * window;
                            }
                        }
                        
                        // Perform FFT
                        const fft = performFFT(chunk);
                        
                        // Store original spectrogram data (only for first channel and at sampling intervals)
                        if (channel === 0 && frameIndex % framesPerPixel === 0) {
                            const specColumn = [];
                            for (let bin = 0; bin < fftSize / 2; bin++) {
                                const freq = bin * sampleRate / fftSize;
                                if (freq > maxFreqDisplay) break;
                                
                                const real = fft[bin * 2];
                                const imag = fft[bin * 2 + 1];
                                const magnitude = Math.sqrt(real * real + imag * imag);
                                specColumn.push(magnitude);
                            }
                            originalSpecData.push(specColumn);
                        }
                        
                        // Create a copy for converted spectrogram
                        const convertedFFT = new Float32Array(fft.length);
                        for (let j = 0; j < fft.length; j++) {
                            convertedFFT[j] = 0; // Start with silence
                        }
                        
                        // Convert linear spectrum to logarithmic frequency bins
                        const minFreq = 20; // Minimum frequency (Hz)
                        const maxFreq = sampleRate / 2; // Nyquist frequency
                        const numLogBins = 200; // Number of logarithmic bins
                        const logBins = [];
                        
                        // Create logarithmic frequency bins
                        for (let i = 0; i < numLogBins; i++) {
                            const logFreq = minFreq * Math.pow(maxFreq / minFreq, i / (numLogBins - 1));
                            const bin = Math.round(logFreq * fftSize / sampleRate);
                            
                            if (bin < fftSize / 2) {
                                const real = fft[bin * 2];
                                const imag = fft[bin * 2 + 1];
                                const magnitude = Math.sqrt(real * real + imag * imag);
                                const phase = Math.atan2(imag, real);
                                
                                logBins.push({
                                    freq: logFreq,
                                    bin: bin,
                                    magnitude: magnitude,
                                    phase: phase,
                                    real: real,
                                    imag: imag
                                });
                            }
                        }
                        
                        // Detect frequency clusters within 20 cents in logarithmic space
                        const clusters = [];
                        let currentCluster = null;
                        const magnitudeThreshold = 0.001;
                        const centsThreshold = 20; // 20 cents
                        
                        for (const logBin of logBins) {
                            if (logBin.magnitude > magnitudeThreshold) {
                                // Check if this frequency is within 20 cents of current cluster
                                let withinCluster = false;
                                if (currentCluster) {
                                    const lastFreq = currentCluster.lastFreq;
                                    // Calculate cents difference: 1200 * log2(f2/f1)
                                    const centsDiff = Math.abs(1200 * Math.log2(logBin.freq / lastFreq));
                                    withinCluster = centsDiff <= centsThreshold;
                                }
                                
                                if (!currentCluster || !withinCluster) {
                                    // Start new cluster
                                    if (currentCluster) {
                                        clusters.push(currentCluster);
                                    }
                                    currentCluster = {
                                        bins: [logBin.bin],
                                        frequencies: [logBin.freq],
                                        magnitudes: [logBin.magnitude],
                                        phases: [logBin.phase],
                                        reals: [logBin.real],
                                        imags: [logBin.imag],
                                        lastFreq: logBin.freq,
                                        count: 1
                                    };
                                } else {
                                    // Add to current cluster
                                    currentCluster.bins.push(logBin.bin);
                                    currentCluster.frequencies.push(logBin.freq);
                                    currentCluster.magnitudes.push(logBin.magnitude);
                                    currentCluster.phases.push(logBin.phase);
                                    currentCluster.reals.push(logBin.real);
                                    currentCluster.imags.push(logBin.imag);
                                    currentCluster.lastFreq = logBin.freq;
                                    currentCluster.count++;
                                }
                            }
                        }
                        if (currentCluster) {
                            clusters.push(currentCluster);
                        }
                        
                        // Process each cluster with exponential weighting
                        for (const cluster of clusters) {
                            // Calculate weighted center frequency of cluster with exponential weighting
                            let totalMagnitude = 0;
                            let weightedFreqSum = 0;
                            let totalReal = 0;
                            let totalImag = 0;
                            let totalExponentialWeight = 0;
                            
                            for (let i = 0; i < cluster.frequencies.length; i++) {
                                const freq = cluster.frequencies[i];
                                const mag = cluster.magnitudes[i];
                                
                                // Exponential weighting: higher magnitudes get exponentially more weight
                                const expWeight = Math.exp(mag * 2); // Adjust multiplier as needed
                                
                                totalMagnitude += mag;
                                weightedFreqSum += freq * mag * expWeight;
                                totalReal += cluster.reals[i] * expWeight;
                                totalImag += cluster.imags[i] * expWeight;
                                totalExponentialWeight += mag * expWeight;
                            }
                            
                            // Center frequency weighted by magnitude and exponential factor
                            const centerFreq = weightedFreqSum / totalExponentialWeight;
                            const avgPhase = Math.atan2(totalImag, totalReal);
                            
                            // Logarithmic amplification based on cluster size
                            // +10dB for 10 frequencies, +20dB for 100, etc.
                            const dbBoost = 10 * Math.log10(cluster.count);
                            const linearBoost = Math.pow(10, dbBoost / 20); // Convert dB to linear
                            const amplifiedMagnitude = totalMagnitude * linearBoost;
                            
                            // Get processing mode
                            const mode = modeSelect.value;
                            let targetFreq;
                            
                            // First, apply interval mapping if any mappings are defined
                            let processedFreq = centerFreq;
                            if (intervalMappings.length > 0) {
                                const centsFromRef = 1200 * Math.log2(centerFreq / initialHz);
                                const mappedCents = applyIntervalMapping(centsFromRef, intervalMappings);
                                processedFreq = initialHz * Math.pow(2, mappedCents / 1200);
                            }
                            
                            // Check if both EDOs are in 10-14 range for scale mode
                            const inValidRange = (fromEdo >= 10 && fromEdo <= 14 && toEdo >= 10 && toEdo <= 14);
                            
                            if (mode === 'approximate' || !inValidRange) {
                                // Approximate mode: Round to nearest pitch in target EDO or custom scale
                                // Also used when outside 10-14 EDO range
                                if (customScale) {
                                    // Use custom scale
                                    targetFreq = findNearestScalePitch(processedFreq, initialHz, customScale);
                                } else {
                                    // Use EDO
                                    const targetStep = freqToEdoStep(processedFreq, initialHz, toEdo);
                                    targetFreq = edoStepToFreq(targetStep, initialHz, toEdo);
                                }
                            } else if (mode === 'scale') {
                                // Scale mode: Transform 12x semitones to (fromEdo/toEdo) * 12x semitones
                                // Only for 10-14 EDO
                                // Convert frequency to semitones from reference
                                const semitones = 12 * Math.log2(processedFreq / initialHz);
                                // Scale by ratio fromEdo/toEdo
                                const scaledSemitones = semitones * (fromEdo / toEdo);
                                // Convert back to frequency
                                targetFreq = initialHz * Math.pow(2, scaledSemitones / 12);
                            }
                            
                            const newBin = Math.round(targetFreq * fftSize / sampleRate);
                            
                            if (newBin >= 0 && newBin < fftSize / 2) {
                                // Place entire cluster energy at target bin with amplification
                                convertedFFT[newBin * 2] += amplifiedMagnitude * Math.cos(avgPhase);
                                convertedFFT[newBin * 2 + 1] += amplifiedMagnitude * Math.sin(avgPhase);
                            }
                        }
                        
                        // Store converted spectrogram data
                        if (channel === 0 && frameIndex % framesPerPixel === 0) {
                            const specColumn = [];
                            for (let bin = 0; bin < fftSize / 2; bin++) {
                                const freq = bin * sampleRate / fftSize;
                                if (freq > maxFreqDisplay) break;
                                
                                const real = convertedFFT[bin * 2];
                                const imag = convertedFFT[bin * 2 + 1];
                                const magnitude = Math.sqrt(real * real + imag * imag);
                                specColumn.push(magnitude);
                            }
                            convertedSpecData.push(specColumn);
                        }
                        
                        // Perform inverse FFT
                        const reconstructed = performIFFT(convertedFFT);
                        
                        frameIndex++;
                        
                        // Overlap-add the chunk to output
                        for (let j = 0; j < fftSize && i + j < length; j++) {
                            const window = 0.5 * (1 - Math.cos(2 * Math.PI * j / fftSize));
                            outputData[i + j] += reconstructed[j] * window;
                        }
                    }
                    
                    // Normalize output
                    let maxVal = 0;
                    for (let i = 0; i < length; i++) {
                        maxVal = Math.max(maxVal, Math.abs(outputData[i]));
                    }
                    if (maxVal > 0) {
                        for (let i = 0; i < length; i++) {
                            outputData[i] /= maxVal * 1.1; // Slight headroom
                        }
                    }
                    
                    outputChannels.push(outputData);
                }

                // Draw spectrograms
                showStatus('Rendering spectrograms...', 'processing');
                drawSpectrogram(origCtx, originalSpecData, spectrogramWidth, spectrogramHeight);
                drawSpectrogram(convCtx, convertedSpecData, spectrogramWidth, spectrogramHeight);

                showStatus('Reconstructing audio...', 'processing');

                // Create output buffer
                const outputBuffer = audioContext.createBuffer(numChannels, length, sampleRate);
                for (let channel = 0; channel < numChannels; channel++) {
                    outputBuffer.copyToChannel(outputChannels[channel], channel);
                }

                // Create downloadable file
                const wavBlob = bufferToWave(outputBuffer, outputBuffer.length);
                const url = URL.createObjectURL(wavBlob);
                
                // Create download link
                const a = document.createElement('a');
                a.href = url;
                const mode = modeSelect.value;
                const scaleLabel = customScale ? 'customscale' : `${fromEdo}edo_to_${toEdo}edo`;
                a.download = `converted_${scaleLabel}_${mode}.wav`;
                a.textContent = 'Download Converted Audio';
                a.style.display = 'block';
                a.style.marginTop = '10px';
                a.style.color = '#4a9eff';
                a.style.textDecoration = 'none';
                a.style.fontWeight = 'bold';
                
                statusDiv.innerHTML = '';
                statusDiv.appendChild(document.createTextNode('Conversion complete! '));
                statusDiv.appendChild(a);
                statusDiv.className = 'status active success';

                // Process color video if not audio-only
                const audioOnly = document.getElementById('audioOnly').checked;
                if (!audioOnly) {
                    statusDiv.appendChild(document.createElement('br'));
                    statusDiv.appendChild(document.createTextNode('Processing color-transformed video...'));
                    statusDiv.className = 'status active processing';
                    await processColorVideo(intervalMappings, customScale, fromEdo, toEdo, initialHz, modeSelect.value);
                } else {
                    showStatus('Audio processing complete!', 'success');
                    statusDiv.innerHTML = '';
                    statusDiv.appendChild(document.createTextNode('Audio processing complete! '));
                    statusDiv.appendChild(a);
                    statusDiv.className = 'status active success';
                }

                processBtn.disabled = false;

            } catch (error) {
                console.error('Error processing audio:', error);
                showStatus(`Error: ${error.message}`, 'error');
                processBtn.disabled = false;
            }
        }

        // Simple FFT implementation (Cooley-Tukey algorithm)
        function performFFT(input) {
            const n = input.length;
            const output = new Float32Array(n * 2); // Real and imaginary pairs
            
            // Copy input to output (real part)
            for (let i = 0; i < n; i++) {
                output[i * 2] = input[i];
                output[i * 2 + 1] = 0;
            }
            
            // Bit-reversal permutation
            let j = 0;
            for (let i = 0; i < n; i++) {
                if (i < j) {
                    // Swap
                    let tempReal = output[i * 2];
                    let tempImag = output[i * 2 + 1];
                    output[i * 2] = output[j * 2];
                    output[i * 2 + 1] = output[j * 2 + 1];
                    output[j * 2] = tempReal;
                    output[j * 2 + 1] = tempImag;
                }
                let k = n / 2;
                while (k <= j) {
                    j -= k;
                    k /= 2;
                }
                j += k;
            }
            
            // Cooley-Tukey FFT
            for (let len = 2; len <= n; len *= 2) {
                const halfLen = len / 2;
                const theta = -2 * Math.PI / len;
                
                for (let i = 0; i < n; i += len) {
                    for (let k = 0; k < halfLen; k++) {
                        const angle = theta * k;
                        const wReal = Math.cos(angle);
                        const wImag = Math.sin(angle);
                        
                        const evenIdx = (i + k) * 2;
                        const oddIdx = (i + k + halfLen) * 2;
                        
                        const tReal = wReal * output[oddIdx] - wImag * output[oddIdx + 1];
                        const tImag = wReal * output[oddIdx + 1] + wImag * output[oddIdx];
                        
                        output[oddIdx] = output[evenIdx] - tReal;
                        output[oddIdx + 1] = output[evenIdx + 1] - tImag;
                        output[evenIdx] += tReal;
                        output[evenIdx + 1] += tImag;
                    }
                }
            }
            
            return output;
        }

        // Inverse FFT
        function performIFFT(input) {
            const n = input.length / 2;
            const conjugate = new Float32Array(input.length);
            
            // Conjugate the input
            for (let i = 0; i < n; i++) {
                conjugate[i * 2] = input[i * 2];
                conjugate[i * 2 + 1] = -input[i * 2 + 1];
            }
            
            // Perform FFT on conjugate
            const result = performFFT(conjugate);
            
            // Conjugate and normalize
            const output = new Float32Array(n);
            for (let i = 0; i < n; i++) {
                output[i] = result[i * 2] / n;
            }
            
            return output;
        }

        // Decode audio from video file
        async function decodeAudioFromVideo(arrayBuffer) {
            try {
                // Try to decode directly (works if browser supports the video's audio codec)
                return await audioContext.decodeAudioData(arrayBuffer.slice(0));
            } catch (error) {
                // If direct decode fails, extract using a trick with an audio element
                return await extractAudioViaElement();
            }
        }

        // Alternative method to extract audio
        async function extractAudioViaElement() {
            return new Promise((resolve, reject) => {
                const audio = new Audio(URL.createObjectURL(videoFile));
                
                audio.addEventListener('loadedmetadata', async () => {
                    try {
                        const mediaSource = audioContext.createMediaElementSource(audio);
                        const destination = audioContext.createMediaStreamDestination();
                        mediaSource.connect(destination);
                        
                        const mediaRecorder = new MediaRecorder(destination.stream);
                        const chunks = [];
                        
                        mediaRecorder.ondataavailable = (e) => chunks.push(e.data);
                        mediaRecorder.onstop = async () => {
                            const blob = new Blob(chunks, { type: 'audio/webm' });
                            const arrayBuffer = await blob.arrayBuffer();
                            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                            resolve(audioBuffer);
                        };
                        
                        mediaRecorder.start();
                        audio.play();
                        
                        setTimeout(() => {
                            audio.pause();
                            mediaRecorder.stop();
                        }, audio.duration * 1000);
                        
                    } catch (error) {
                        reject(error);
                    }
                });
                
                audio.addEventListener('error', reject);
            });
        }

        // Convert AudioBuffer to WAV file
        function bufferToWave(abuffer, len) {
            const numOfChan = abuffer.numberOfChannels;
            const length = len * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let offset = 0;
            let pos = 0;

            // Write WAV header
            setUint32(0x46464952); // "RIFF"
            setUint32(length - 8); // file length - 8
            setUint32(0x45564157); // "WAVE"
            setUint32(0x20746d66); // "fmt " chunk
            setUint32(16); // length = 16
            setUint16(1); // PCM (uncompressed)
            setUint16(numOfChan);
            setUint32(abuffer.sampleRate);
            setUint32(abuffer.sampleRate * 2 * numOfChan); // avg. bytes/sec
            setUint16(numOfChan * 2); // block-align
            setUint16(16); // 16-bit
            setUint32(0x61746164); // "data" - chunk
            setUint32(length - pos - 4); // chunk length

            // Write interleaved data
            for (let i = 0; i < abuffer.numberOfChannels; i++) {
                channels.push(abuffer.getChannelData(i));
            }

            while (pos < len) {
                for (let i = 0; i < numOfChan; i++) {
                    let sample = Math.max(-1, Math.min(1, channels[i][pos]));
                    sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
                pos++;
            }

            return new Blob([buffer], { type: 'audio/wav' });

            function setUint16(data) {
                view.setUint16(offset, data, true);
                offset += 2;
            }

            function setUint32(data) {
                view.setUint32(offset, data, true);
                offset += 4;
            }
        }

        // Process color video where hue represents pitch
        async function processColorVideo(intervalMappings, customScale, fromEdo, toEdo, initialHz, mode) {
            try {
                const canvas = document.getElementById('colorVideoCanvas');
                const ctx = canvas.getContext('2d');
                const colorVideoElement = document.getElementById('colorVideo');
                
                // Create a temporary video element for reading frames
                const tempVideo = document.createElement('video');
                tempVideo.src = videoPlayer.src;
                tempVideo.muted = true;
                
                await new Promise((resolve) => {
                    tempVideo.onloadedmetadata = resolve;
                });
                
                canvas.width = tempVideo.videoWidth;
                canvas.height = tempVideo.videoHeight;
                
                const fps = 30;
                const duration = tempVideo.duration;
                const frameSkip = parseInt(document.getElementById('frameSkip').value);
                const frameInterval = (1/fps) * frameSkip;
                const totalFramesToProcess = Math.floor(duration / frameInterval);
                const frames = [];
                
                showStatus(`Processing video frames (${totalFramesToProcess} frames, skipping ${frameSkip-1} of every ${frameSkip})...`, 'processing');
                
                // Process frames with skipping
                for (let time = 0; time < duration; time += frameInterval) {
                    tempVideo.currentTime = time;
                    await new Promise(resolve => {
                        tempVideo.onseeked = resolve;
                    });
                    
                    // Draw current frame
                    ctx.drawImage(tempVideo, 0, 0);
                    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                    const data = imageData.data;
                    
                    // Transform each pixel's hue
                    for (let i = 0; i < data.length; i += 4) {
                        const r = data[i];
                        const g = data[i + 1];
                        const b = data[i + 2];
                        
                        // Convert to HSV
                        const hsv = rgbToHsv(r, g, b);
                        
                        // Hue represents pitch: 360° = 1200 cents, so 1 cent = 0.3°
                        const inputCents = (hsv.h / 0.3) % 1200;
                        
                        // Transform cents using the same logic as audio
                        const outputCents = transformCents(inputCents, intervalMappings, customScale, fromEdo, toEdo, initialHz, mode);
                        
                        // Convert back to hue
                        const outputHue = (outputCents % 1200) * 0.3;
                        
                        // Convert back to RGB with transformed hue
                        const rgb = hsvToRgb(outputHue, hsv.s, hsv.v);
                        
                        data[i] = rgb.r;
                        data[i + 1] = rgb.g;
                        data[i + 2] = rgb.b;
                    }
                    
                    ctx.putImageData(imageData, 0, 0);
                    
                    // Duplicate frame based on skip rate to maintain timing
                    for (let skip = 0; skip < frameSkip; skip++) {
                        frames.push(canvas.toDataURL('image/jpeg', 0.9));
                    }
                    
                    if (frames.length % 30 === 0) {
                        showStatus(`Processed ${frames.length / frameSkip} unique frames (${frames.length} total with duplication)...`, 'processing');
                    }
                }
                
                showStatus('Encoding video...', 'processing');
                
                // Create video from frames using MediaRecorder
                const stream = canvas.captureStream(fps);
                const mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'video/webm',
                    videoBitsPerSecond: 5000000
                });
                
                const chunks = [];
                mediaRecorder.ondataavailable = (e) => {
                    if (e.data.size > 0) {
                        chunks.push(e.data);
                    }
                };
                
                await new Promise((resolve) => {
                    mediaRecorder.onstop = resolve;
                    mediaRecorder.start();
                    
                    // Play through frames at original fps to maintain duration
                    let frameIndex = 0;
                    const playInterval = setInterval(() => {
                        if (frameIndex >= frames.length) {
                            clearInterval(playInterval);
                            mediaRecorder.stop();
                            return;
                        }
                        
                        const img = new Image();
                        img.onload = () => {
                            ctx.drawImage(img, 0, 0);
                        };
                        img.src = frames[frameIndex];
                        frameIndex++;
                    }, 1000 / fps);
                });
                
                const videoBlob = new Blob(chunks, { type: 'video/webm' });
                const videoUrl = URL.createObjectURL(videoBlob);
                
                // Wait for video to load before showing
                colorVideoElement.src = videoUrl;
                await new Promise(resolve => {
                    colorVideoElement.onloadeddata = resolve;
                });
                
                document.getElementById('colorVideoWrapper').classList.add('active');
                
                // Create download link
                const downloadLink = document.createElement('a');
                downloadLink.href = videoUrl;
                downloadLink.download = 'color_transformed_video.webm';
                downloadLink.textContent = 'Download Color Video';
                downloadLink.style.display = 'block';
                downloadLink.style.marginTop = '5px';
                downloadLink.style.color = '#4a9eff';
                downloadLink.style.textDecoration = 'none';
                downloadLink.style.fontWeight = 'bold';
                
                // Update status with both download links
                const audioLink = statusDiv.querySelector('a');
                statusDiv.innerHTML = '';
                statusDiv.appendChild(document.createTextNode('All processing complete! '));
                if (audioLink) {
                    statusDiv.appendChild(audioLink);
                    statusDiv.appendChild(document.createTextNode(' | '));
                }
                statusDiv.appendChild(downloadLink);
                statusDiv.className = 'status active success';
                
            } catch (error) {
                console.error('Error processing color video:', error);
                showStatus(`Video processing error: ${error.message}`, 'error');
            }
        }

        // Process button click handler
        processBtn.addEventListener('click', processAudio);
    </script>
</body>
</html>
